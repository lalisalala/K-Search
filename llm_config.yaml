llm:
  model_name: "llama2:13b"
  temperature: 0
  max_tokens: 2000
  api_url: "http://localhost:11434/api/generate"
faiss:
  top_k: 10
logging:
  level: "INFO"
  file: "app.log"
